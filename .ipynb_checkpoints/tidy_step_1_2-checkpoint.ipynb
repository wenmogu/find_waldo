{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1920sem1\\cs4243\\cs4243-lab3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random', 'shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as itertools \n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.spatial as scipy_spatial\n",
    "from skimage import color\n",
    "import pickle\n",
    "import sklearn\n",
    "import cyvlfeat as vlfeat\n",
    "import math\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from utils import *\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_save_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\rectangles'\n",
    "\n",
    "image_dir = \"D:\\\\1920Sem1\\CS4243\\project\\CS4243-Project\\datasets\\JPEGImages\"\n",
    "images = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, img))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_input_img(img_0):\n",
    "    hsv = cv2.cvtColor(img_0, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    lower_red = np.array([110,80,80])\n",
    "    upper_red = np.array([160,255,255])\n",
    "    mask_red_only = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "    lower_white = np.array([0,0,255])\n",
    "    upper_white = np.array([255,255,255])\n",
    "    mask_white_only = cv2.inRange(hsv, lower_white, upper_white)\n",
    "    \n",
    "    # attach a column of row number to the masks\n",
    "    index_array = np.flip(np.arange(mask_red_only.shape[1]))\n",
    "\n",
    "    stacked_red = np.vstack((index_array, mask_red_only))\n",
    "    stacked_red_white = np.vstack((stacked_red, mask_white_only))\n",
    "\n",
    "    red_mask_rot = np.rot90(stacked_red)\n",
    "    red_white_mask_rot = np.rot90(stacked_red_white)\n",
    "    \n",
    "    return red_white_mask_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suspected_waldo_stripe_region_for_col(red_white_mask_col, ratio_range=(1/2, 2), white_pixel_threshold=0.15):\n",
    "    col_index = red_white_mask_col[0]\n",
    "    red_mask_col = red_white_mask_col[1:int((len(red_white_mask_col)-1)/2+1)]\n",
    "    white_mask_col = red_white_mask_col[int((len(red_white_mask_col)-1)/2+1):]\n",
    "    assert red_mask_col.shape == white_mask_col.shape, \"the mask column inputs for white and red masks are of different shapes:\" + str(red_mask_col.shape) + \"; \" + str(white_mask_col.shape)\n",
    "    \n",
    "    \n",
    "    white_mask_col = white_mask_col[1:]\n",
    "    \n",
    "    new_red_mask_col = np.zeros(len(red_mask_col))\n",
    "    new_red_mask_col[red_mask_col == 255] = 1\n",
    "    new_red_mask_col[red_mask_col == 0] = -1\n",
    "    \n",
    "    new_white_mask_col = np.zeros(len(white_mask_col))\n",
    "    new_white_mask_col[white_mask_col == 255] = 1\n",
    "    new_white_mask_col[white_mask_col == 0] = 0\n",
    "\n",
    "    streak_array = get_streak_len_array_with_approx(new_red_mask_col, 0)\n",
    "    ratio_array = np.array([abs(streak_array[i]) / abs(streak_array[i + 1]) for i in range(len(streak_array) - 1)])\n",
    "    accepted_ratio_indice, = np.where(np.logical_and(ratio_array > ratio_range[0], ratio_array < ratio_range[1]))\n",
    "    is_accepted_ratio_indice_continuous, lst_of_start_and_end = is_indice_continuous(accepted_ratio_indice)\n",
    "\n",
    "    revised_lst_of_start_and_end = []\n",
    "\n",
    "    if is_accepted_ratio_indice_continuous:\n",
    "        # then check if the negative pixels are white pixels\n",
    "        for start, end in lst_of_start_and_end:\n",
    "            is_region_start_with_red = streak_array[start+2] > 0\n",
    "            offset = 0\n",
    "            if is_region_start_with_red:\n",
    "                offset = 1\n",
    "            \n",
    "            total_number_of_white_pixels = 0\n",
    "            total_number_of_non_red_pixels = 0\n",
    "#             region = [np.sum(np.abs(streak_array[:accepted_ratio_indice[start]])), np.sum(np.abs(streak_array[:accepted_ratio_indice[end-1]+1]))]\n",
    "#             print(\"start, end: \" + str(region[0]) + \", \" + str(region[1]))\n",
    "\n",
    "            while start + offset < end:\n",
    "                supposedly_white_pixel_region = [np.sum(np.abs(streak_array[:accepted_ratio_indice[start + offset]])), np.sum(np.abs(streak_array[:accepted_ratio_indice[start + offset]+1]))]\n",
    "\n",
    "                number_of_white_pixels = np.sum(new_white_mask_col[int(supposedly_white_pixel_region[0]) : int(supposedly_white_pixel_region[1])])\n",
    "                number_of_non_red_pixels = supposedly_white_pixel_region[1] - supposedly_white_pixel_region[0]\n",
    "\n",
    "                total_number_of_white_pixels += number_of_white_pixels\n",
    "                total_number_of_non_red_pixels += number_of_non_red_pixels\n",
    "                offset += 2\n",
    "\n",
    "            if total_number_of_white_pixels / total_number_of_non_red_pixels > white_pixel_threshold:\n",
    "                revised_lst_of_start_and_end.append((start, end))\n",
    "        \n",
    "        if len(revised_lst_of_start_and_end) == 0:\n",
    "            empty_terms = np.array(list(itertools.repeat([int(False), col_index, -1, -1, 0, 0], 20)))\n",
    "            return empty_terms\n",
    "        stripe_region = [[int(True), \\\n",
    "                          col_index, \\\n",
    "                          np.sum(np.abs(streak_array[:accepted_ratio_indice[start]])), \\\n",
    "                          np.sum(np.abs(streak_array[:accepted_ratio_indice[end-1]+1])), \\\n",
    "                          int(end - start), \\\n",
    "                          (np.sum(np.abs(streak_array[:accepted_ratio_indice[end-1]+1])) - np.sum(np.abs(streak_array[:accepted_ratio_indice[start]]))) / int(end - start)] \\\n",
    "                         for start, end in revised_lst_of_start_and_end]\n",
    "        # from here examine the percentage of white pixels in the region\n",
    "        # if there are white pixels dominating the negative pixels, then it is stripes\n",
    "        # return (1, col_number, start_of_stripe_region, end_of_stripe_region, number_of_stripes, average_width_of_stripes)        \n",
    "        how_many_more_terms = 20 - len(stripe_region)\n",
    "        additional_terms = np.array(list(itertools.repeat([int(False), col_index, -1, -1, 0, 0], how_many_more_terms)))\n",
    "        return np.vstack((stripe_region, additional_terms))    \n",
    "\n",
    "    empty_terms = np.array(list(itertools.repeat([int(False), col_index, -1, -1, 0, 0], 20)))\n",
    "    return empty_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_streak_len_array_with_approx(unit_lst, threshold):\n",
    "    # in the unit_lst, 1 represents the superior pixel value, -1 is the inferior pixel value\n",
    "    # the superior pixel value is the one that is well captured in color distillation \n",
    "    # in our case e.g. red is the superior pixel value because it is well captured \n",
    "    # and white is the inferior pixel value because it might be noise\n",
    "    # we want to restore the shape outlined by the superior pixel value\n",
    "    # hence we need to minimize the disturbannce by the inferior pixel value\n",
    "    # hence we need to remove the 'sparse' inferior pixel value located within the streaks of superior pixel values\n",
    "    # the definition of sparse is given by the threshold\n",
    "    previous = 0\n",
    "    streak_len_array = []\n",
    "    streak_len = 0\n",
    "    fast_forward_count = 0\n",
    "    for i in range(len(unit_lst)):\n",
    "        if i == 0:\n",
    "            previous = unit_lst[i]\n",
    "            \n",
    "        if fast_forward_count > 0:\n",
    "            fast_forward_count -= 1\n",
    "            if i == len(unit_lst) - 1:\n",
    "                streak_len_array.append(streak_len)\n",
    "            continue\n",
    "        \n",
    "        if previous == unit_lst[i]:\n",
    "            cur_streak_len = streak_len\n",
    "            streak_len += unit_lst[i]\n",
    "            if i == len(unit_lst) - 1:\n",
    "                streak_len_array.append(streak_len)\n",
    "            previous = unit_lst[i]\n",
    "        else:\n",
    "            if streak_len > 0:\n",
    "                # superior pixel streak ending\n",
    "                cur_streak_len = streak_len\n",
    "                # we need to look ahead the threshold number of pixels to see if theres any superior pixel\n",
    "                next_few_number = min(threshold, len(unit_lst) - 1 - i)\n",
    "                next_few_pixels = unit_lst[i : i+1+next_few_number]\n",
    "                # if there is we will continue the superior pixel streak from there\n",
    "                indice_of_next_superior_pixel, = np.where(next_few_pixels == 1)\n",
    "                if len(indice_of_next_superior_pixel) != 0:\n",
    "                    next_superior = max(indice_of_next_superior_pixel)\n",
    "                    streak_len += 1 + next_superior\n",
    "                    fast_forward_count = next_superior\n",
    "                    previous = abs(unit_lst[i])\n",
    "                # if not we end the superior pixel streak here and start a new inferior pixel streak\n",
    "                else:\n",
    "                    streak_len_array.append(cur_streak_len)\n",
    "                    streak_len = 0\n",
    "                    streak_len += unit_lst[i]\n",
    "                    previous = unit_lst[i]\n",
    "            elif streak_len < 0:\n",
    "                # inferior pixel streak ending\n",
    "                # we will start a new superior pixel streak\n",
    "                streak_len_array.append(streak_len)\n",
    "                streak_len = 0\n",
    "                streak_len += unit_lst[i]\n",
    "                previous = unit_lst[i]\n",
    "            \n",
    "    return streak_len_array\n",
    "\n",
    "def get_streak_len_array(unit_lst):\n",
    "    previous = 0\n",
    "    streak_len_array = []\n",
    "    streak_len = 0\n",
    "    for i in range(len(unit_lst)):\n",
    "        if i == 0:\n",
    "            previous = unit_lst[i]\n",
    "        \n",
    "        if previous == unit_lst[i]:\n",
    "            streak_len += unit_lst[i]\n",
    "            if i == len(unit_lst) - 1:\n",
    "                streak_len_array.append(streak_len)\n",
    "        else:\n",
    "            streak_len_array.append(streak_len)\n",
    "            streak_len = 0\n",
    "            \n",
    "        previous = unit_lst[i]\n",
    "        \n",
    "    return streak_len_array\n",
    "\n",
    "\n",
    "# Method: Given a 1D array, find the starting position and the ending position of an arithmatic sequence with the next equal to 1+previous. The sequence must contain no less than 3 numbers.\n",
    "# Input: a 1D array of integers\n",
    "# Output: a tuple in the form of (Boolean, list). The boolean is True when there is at least one sequence. The list consists of tuples (starting_position_of_sequence_inclusive, ending_position_of_sequence_exclusive)\n",
    "def is_indice_continuous(lst_of_indice):\n",
    "    # right now the number of stripes accepted is 3\n",
    "    is_continuous = False\n",
    "    continuous_count = 0\n",
    "    prev = -1\n",
    "    start_index = -1\n",
    "    lst_of_start_and_end = []\n",
    "    for i in range(len(lst_of_indice)):\n",
    "        if i == 0:\n",
    "            prev = lst_of_indice[i]\n",
    "            start_index = i\n",
    "            continuous_count = 1\n",
    "            continue\n",
    "            \n",
    "        if lst_of_indice[i] - prev == 1:\n",
    "            # the streak continues\n",
    "            continuous_count += 1\n",
    "        else:\n",
    "            # the streak ends\n",
    "            if continuous_count >= 3:\n",
    "                lst_of_start_and_end.append((start_index, i))\n",
    "                is_continuous = True\n",
    "\n",
    "            continuous_count = 1\n",
    "            start_index = i\n",
    "            \n",
    "        if i == len(lst_of_indice) - 1:\n",
    "            if start_index != -1 and continuous_count >= 3:\n",
    "                lst_of_start_and_end.append((start_index, i))\n",
    "                is_continuous = True\n",
    "        \n",
    "        prev = lst_of_indice[i]\n",
    "        \n",
    "    return (is_continuous, lst_of_start_and_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "def convert_rot90_col_no_to_original_col_no(rot90_image, col_no):\n",
    "    return rot90_image.shape[0] - col_no\n",
    "\n",
    "def convert_original_col_no_to_rot90_col_no(image, col_no):\n",
    "    return image.shape[1] - col_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_able_to_combine(row1, row2, confidence_area_overlap_ratio_threshold):\n",
    "    row1_width = row1[3] - row1[2]\n",
    "    row1_height = row1[5] - row1[4]\n",
    "    row2_width = row2[3] - row2[2]\n",
    "    row2_height = row2[5] - row2[4]\n",
    "    \n",
    "    bbox1 = (row1[2], row1[4], row1_width, row1_height)\n",
    "    bbox2 = (row2[2], row2[4], row2_width, row2_height)\n",
    "    \n",
    "    iou = IoU(bbox1, bbox2)\n",
    "    \n",
    "    if (iou >= confidence_area_overlap_ratio_threshold):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def IoU(bbox1, bbox2):\n",
    "    \"\"\" Compute IoU of two bounding boxes\n",
    "\n",
    "    Args:\n",
    "        bbox1 - 4-tuple (x, y, w, h) where (x, y) is the top left corner of\n",
    "            the bounding box, and (w, h) are width and height of the box.\n",
    "        bbox2 - 4-tuple (x, y, w, h) where (x, y) is the top left corner of\n",
    "            the bounding box, and (w, h) are width and height of the box.\n",
    "    Returns:\n",
    "        score - IoU score\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    score = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    x_left = max(x1, x2)\n",
    "    y_top = max(y1, y2)\n",
    "    x_right = min(x1+w1, x2+w2)\n",
    "    y_bottom = min(y1+h1, y2+h2)\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    area1 = w1 * h1\n",
    "    area2 = w2 * h2\n",
    "    score = intersection_area / float(area1 + area2 - intersection_area)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return score\n",
    "\n",
    "    \n",
    "def merge_rows(data, row_1_index, row_2_index):\n",
    "#     ['col_no_original', 'stripe_centre_for_each_col',\n",
    "#        'col_confidence_interval_start', 'col_confidence_interval_end',\n",
    "#        'row_confidence_interval_start', 'row_confidence_interval_end']\n",
    "    row_1 = data[row_1_index][:-1]\n",
    "    row_2 = data[row_2_index][:-1]\n",
    "    row_1_no_of_times_combined = data[row_1_index][-1]\n",
    "    row_2_no_of_times_combined = data[row_2_index][-1]\n",
    "    total_no_of_times_combined = row_2_no_of_times_combined + row_1_no_of_times_combined\n",
    "    row_3 = np.add(row_1*row_1_no_of_times_combined, row_2*row_2_no_of_times_combined) / total_no_of_times_combined\n",
    "    \n",
    "    row_3 = np.append(row_3, total_no_of_times_combined)\n",
    "#     new_data = np.append(data, row_3.reshape((1, len(row_3))), axis=0)\n",
    "#     new_data = np.delete(new_data, [row_1_index, row_2_index], axis=0)\n",
    "    data[row_1_index] = row_3\n",
    "    new_data = np.delete(data, [row_2_index], axis=0)\n",
    "    return new_data\n",
    "\n",
    "def combine_to_region(df, confidence_area_overlap_ratio_threshold):\n",
    "    if (\"no_of_times_combined\" not in df.columns):\n",
    "        df[\"no_of_times_combined\"] = 1\n",
    "        \n",
    "    all_data = df[:].to_numpy()\n",
    "    \n",
    "    data = np.copy(all_data)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while (True):\n",
    "        if count >= len(data) - 1:\n",
    "            return pd.DataFrame(data=data, columns=df.columns)\n",
    "        else:\n",
    "            this_row = data[count]\n",
    "            rest_rows = np.delete(data, count, axis=0)\n",
    "            this_vector = this_row[:2]\n",
    "            rest_vectors = rest_rows[:, :2]\n",
    "            # find the vector with the shortest distance\n",
    "            dist = scipy_spatial.distance.cdist(this_vector.reshape((1,2)), rest_vectors)\n",
    "            closest_vector_index = np.argmin(dist)\n",
    "            closest_row = rest_rows[closest_vector_index]\n",
    "            # if can combine, then merge, and the index will take the one of the smaller index, recurse.\n",
    "            can_combine = is_able_to_combine(this_row, closest_row, confidence_area_overlap_ratio_threshold)\n",
    "            if can_combine:\n",
    "                # merge, and the index will take the one of the smaller index, recurse.\n",
    "                if count <= closest_vector_index:\n",
    "                    closest_vector_index += 1\n",
    "                new_data = merge_rows(data, count, closest_vector_index)\n",
    "                count = 0\n",
    "                data = new_data\n",
    "            else:\n",
    "                # count++, recurse.\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_raw_stripe_confidence_regions_df_from_suspected_stripe_regions(result):\n",
    "    col_confidence_interval_width = 8\n",
    "    row_confidence_interval_width = 10\n",
    "    reshaped = np.reshape(result, (result.shape[0] * result.shape[1], result.shape[2]))\n",
    "    df = pd.DataFrame(data=reshaped, columns=[\"has_stripes\", \"col_no\", \"stripe_start\", \"stripe_end\", \"no_of_stripes\", \"average_stripe_width\"])\n",
    "    df = df[df[\"has_stripes\"]==1]\n",
    "    df = df[df[\"average_stripe_width\"]>=2]\n",
    "    tidy_df = pd.DataFrame()\n",
    "    tidy_df[\"col_no_original\"] = convert_rot90_col_no_to_original_col_no(np.rot90(img_0), df[\"col_no\"])\n",
    "    tidy_df[\"stripe_centre_for_each_col\"] = (df[\"stripe_start\"] + df[\"stripe_end\"]) / 2\n",
    "\n",
    "    tidy_df[\"col_confidence_interval_start\"] = tidy_df[\"col_no_original\"] - df[\"average_stripe_width\"] * col_confidence_interval_width / 2\n",
    "    tidy_df[\"col_confidence_interval_start\"] = np.where(tidy_df[\"col_confidence_interval_start\"]< 0, 0, tidy_df[\"col_confidence_interval_start\"]) \n",
    "\n",
    "    tidy_df[\"col_confidence_interval_end\"] = tidy_df[\"col_no_original\"] + df[\"average_stripe_width\"] * col_confidence_interval_width / 2\n",
    "    tidy_df[\"col_confidence_interval_end\"] = np.where(tidy_df[\"col_confidence_interval_end\"]>img_0.shape[1]-1, img_0.shape[1]-1, tidy_df[\"col_confidence_interval_end\"]) \n",
    "\n",
    "    tidy_df[\"row_confidence_interval_start\"] = tidy_df[\"stripe_centre_for_each_col\"] - df[\"average_stripe_width\"] * row_confidence_interval_width / 2\n",
    "    tidy_df[\"row_confidence_interval_start\"] = np.where(tidy_df[\"row_confidence_interval_start\"]< 0, 0, tidy_df[\"row_confidence_interval_start\"]) \n",
    "\n",
    "    tidy_df[\"row_confidence_interval_end\"] = tidy_df[\"stripe_centre_for_each_col\"] + df[\"average_stripe_width\"] * row_confidence_interval_width / 2\n",
    "    tidy_df[\"row_confidence_interval_end\"] = np.where(tidy_df[\"row_confidence_interval_end\"]>img_0.shape[0]-1, img_0.shape[0]-1, tidy_df[\"row_confidence_interval_end\"]) \n",
    "    \n",
    "    return tidy_df\n",
    "\n",
    "def get_combined_stripe_confidence_regions_df_from_raw_stripe_confidence_regions_df(raw_df, combine_threshold=0.2):\n",
    "    return combine_to_region(raw_df, combine_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_rectangles_from_stripe_region_df(df, img, img_name):\n",
    "    df_for_rectangles = df[['row_confidence_interval_start', 'col_confidence_interval_start', 'row_confidence_interval_end', 'col_confidence_interval_end']]\n",
    "    rectangles = df_for_rectangles[:].to_numpy().astype(int)\n",
    "    img_copy = np.copy(img)\n",
    "    count = 0\n",
    "    for rect in rectangles:\n",
    "        cv2.rectangle(img_copy, (rect[1], rect[0]), (rect[3], rect[2]), color=(0, 255, 0), thickness=16)\n",
    "        count += 1\n",
    "        \n",
    "    img_full_name = img_name + \"_\" + str(count) + \".jpeg\"\n",
    "    cv2.imwrite(os.path.join(img_save_dir, img_full_name), img_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range_square_save_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\range_square'\n",
    "range_save_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\range'\n",
    "\n",
    "def match_wanda_face(template_original, range_to_match, kernel_size, range_name):\n",
    "    template = cv2.resize(template_original, (kernel_size, kernel_size))\n",
    "    \n",
    "    w, h = template.shape[::-1]\n",
    "    res = cv2.matchTemplate(range_to_match, template, cv2.TM_CCOEFF_NORMED)\n",
    "    res_max_value = res[np.unravel_index(np.argmax(res, axis=None), res.shape)]\n",
    "#     print(kernel_size)\n",
    "#     print(range_name + \": \" + str(res_max_value))\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "    top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "    \n",
    "    range_to_match_copy = range_to_match.copy()\n",
    "    \n",
    "    square_cropped = range_to_match_copy[top_left[1]:top_left[1] + h , top_left[0]:top_left[0] + w ]\n",
    "    imgsquare_name = \"square\" + \"_\" + range_name + \"_\" + str(int(res_max_value*100)) + \".png\"\n",
    "    cv2.imwrite(os.path.join(range_square_save_dir, imgsquare_name), square_cropped)\n",
    "\n",
    "#     cv2.rectangle(range_to_match_copy,top_left, bottom_right, 255, 2)   \n",
    "    imgname = range_name + \"_\" + str(int(res_max_value*100)) + \".png\"\n",
    "    cv2.imwrite(os.path.join(range_save_dir, imgname), range_to_match_copy)\n",
    "    \n",
    "#     print(square_cropped.shape)\n",
    "    return square_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_files_in_dir(dir):\n",
    "    return [os.path.join(dir, img) for img in os.listdir(dir) if os.path.isfile(os.path.join(dir, img))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range_square_save_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\range_square'\n",
    "range_save_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\range'\n",
    "\n",
    "template_wenda_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\range_square\\\\wenda'\n",
    "def match_wanda_face_dir(template_original_dir, range_to_match, kernel_size, range_name):\n",
    "    template_originals = all_files_in_dir(template_original_dir)[:3]\n",
    "    \n",
    "    range_to_match_copy = range_to_match.copy()\n",
    "    range_to_match_clean = range_to_match.copy()\n",
    "    \n",
    "    bboxes = []\n",
    "    squares_cropped = []\n",
    "    res_values = []\n",
    "    for template_original_address in template_originals:\n",
    "        template_original = cv2.imread(template_original_address, 0)\n",
    "        w = template_original.shape[0]\n",
    "        \n",
    "        template = cv2.resize(template_original, (kernel_size, kernel_size))\n",
    "\n",
    "        w, h = template.shape[::-1]\n",
    "        res = cv2.matchTemplate(range_to_match, template, cv2.TM_CCOEFF_NORMED)\n",
    "        res_max_value = res[np.unravel_index(np.argmax(res, axis=None), res.shape)]\n",
    "    #     print(kernel_size)\n",
    "    #     print(range_name + \": \" + str(res_max_value))\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "        top_left = max_loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "        \n",
    "        bbox = (top_left[0], top_left[1], w, h)\n",
    "        bboxes.append(bbox)\n",
    "        \n",
    "        square_cropped = range_to_match_clean[top_left[1]:top_left[1] + h , top_left[0]:top_left[0] + w ]\n",
    "        squares_cropped.append(square_cropped)\n",
    "        res_values.append(res_max_value)\n",
    "\n",
    "        cv2.rectangle(range_to_match_copy,top_left, bottom_right, 255, 2)   \n",
    "    \n",
    "    iou_3 = IoU_3(bboxes[0], bboxes[1], bboxes[2])\n",
    "    \n",
    "    if (iou_3 > -0.2):\n",
    "#         ind = res_values.index(max(res_values))\n",
    "        for k in range(3):\n",
    "            square_cropped = squares_cropped[k]\n",
    "            imgsquare_name = \"square\" + str(k) + \"_\" + range_name + \"_\" + str(int(res_max_value*100)) + \".png\"\n",
    "            cv2.imwrite(os.path.join(range_square_save_dir, imgsquare_name), square_cropped)\n",
    "        \n",
    "        imgname = range_name + \"_\" + str(int(res_max_value*100)) + \".png\"\n",
    "        cv2.imwrite(os.path.join(range_save_dir, imgname), range_to_match_copy)\n",
    "\n",
    "    #     print(square_cropped.shape)\n",
    "#         return square_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    #     print(square_cropped.shape)\n",
    "#         return square_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU_3(bbox1, bbox2, bbox3):\n",
    "    \"\"\" Compute IoU of two bounding boxes\n",
    "\n",
    "    Args:\n",
    "        bbox1 - 4-tuple (x, y, w, h) where (x, y) is the top left corner of\n",
    "            the bounding box, and (w, h) are width and height of the box.\n",
    "        bbox2 - 4-tuple (x, y, w, h) where (x, y) is the top left corner of\n",
    "            the bounding box, and (w, h) are width and height of the box.\n",
    "        bbox3 - 4-tuple (x, y, w, h) where (x, y) is the top left corner of\n",
    "            the bounding box, and (w, h) are width and height of the box.\n",
    "    Returns:\n",
    "        score - IoU score\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    x3, y3, w3, h3 = bbox3\n",
    "    score = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    y1y2_x_left = max(x1, x2)\n",
    "    y1y2_y_top = max(y1, y2)\n",
    "    y1y2_x_right = min(x1+w1, x2+w2)\n",
    "    y1y2_y_bottom = min(y1+h1, y2+h2)\n",
    "    \n",
    "    if y1y2_x_right < y1y2_x_left or y1y2_y_bottom < y1y2_y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    y1y3_x_left = max(x1, x3)\n",
    "    y1y3_y_top = max(y1, y3)\n",
    "    y1y3_x_right = min(x1+w1, x3+w3)\n",
    "    y1y3_y_bottom = min(y1+h1, y3+h3)\n",
    "    \n",
    "    if y1y3_x_right < y1y3_x_left or y1y3_y_bottom < y1y3_y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    y3y2_x_left = max(x3, x2)\n",
    "    y3y2_y_top = max(y3, y2)\n",
    "    y3y2_x_right = min(x3+w3, x2+w2)\n",
    "    y3y2_y_bottom = min(y3+h3, y2+h2)\n",
    "\n",
    "    if y3y2_x_right < y3y2_x_left or y3y2_y_bottom < y3y2_y_top:\n",
    "        return 0.0\n",
    "  \n",
    "    y1y2y3_x_left = max(x1, x2, y3)\n",
    "    y1y2y3_y_top = max(y1, y2, y3)\n",
    "    y1y2y3_x_right = min(x1+w1, x2+w2, x3+w3)\n",
    "    y1y2y3_y_bottom = min(y1+h1, y2+h2, y3+h3)\n",
    "\n",
    "    if y1y2y3_x_right < y1y2y3_x_left or y1y2y3_y_bottom < y1y2y3_y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    y1y2_intersection_area = (y1y2_x_right - y1y2_x_left) * (y1y2_y_bottom - y1y2_y_top)\n",
    "    y1y3_intersection_area = (y1y3_x_right - y1y3_x_left) * (y1y3_y_bottom - y1y3_y_top)\n",
    "    y3y2_intersection_area = (y3y2_x_right - y3y2_x_left) * (y3y2_y_bottom - y3y2_y_top)\n",
    "\n",
    "    y1y2y3_intersection_area = (y1y2y3_x_right - y1y2y3_x_left) * (y1y2y3_y_bottom - y1y2y3_y_top)\n",
    "    \n",
    "    y1_area1 = w1 * h1\n",
    "    y2_area2 = w2 * h2\n",
    "    y3_area3 = w3 * h3\n",
    "\n",
    "    total_area = float(y1_area1 + y2_area2 + y3_area3 - y1y2_intersection_area - y1y3_intersection_area - y3y2_intersection_area + y1y2y3_intersection_area)\n",
    "    score = y1y2y3_intersection_area / total_area\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suspected_waldo_stripe_region_for_img(img):\n",
    "\n",
    "    red_white_mask_rot = preprocess_input_img(img)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result  = np.apply_along_axis(get_suspected_waldo_stripe_region_for_col, 1, red_white_mask_rot)\n",
    "    stop_time = time.time()\n",
    "    print(\"--- %s seconds ---\" % (stop_time - start_time))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_face(template_original_dir, img, row, kernel_size, range_name, square_cropped_save_dir):\n",
    "    \n",
    "    row_start, col_start, row_end, col_end, stripe_width = row\n",
    "    \n",
    "    range1 = img[row_start:row_end, col_start:col_end]\n",
    "    range_to_match = cv2.cvtColor(range1, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    template_originals = all_files_in_dir(template_original_dir)[:3]\n",
    "    \n",
    "    range_to_match_copy = range_to_match.copy()\n",
    "    range_to_match_clean = range_to_match.copy()\n",
    "    \n",
    "    bboxes = []\n",
    "    squares_cropped = []\n",
    "#     res_values = []\n",
    "    for template_original_address in template_originals:\n",
    "        template_original = cv2.imread(template_original_address, 0)\n",
    "        w = template_original.shape[0]\n",
    "        \n",
    "        template = cv2.resize(template_original, (kernel_size, kernel_size))\n",
    "\n",
    "        w, h = template.shape[::-1]\n",
    "        res = cv2.matchTemplate(range_to_match, template, cv2.TM_CCOEFF_NORMED)\n",
    "        res_max_value = res[np.unravel_index(np.argmax(res, axis=None), res.shape)]\n",
    "    #     print(kernel_size)\n",
    "    #     print(range_name + \": \" + str(res_max_value))\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "        top_left = max_loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "        \n",
    "        bbox = (col_start + top_left[0], row_start + top_left[1], w, h)\n",
    "        bboxes.append(bbox)\n",
    "        \n",
    "        square_cropped = range_to_match_clean[top_left[1]:top_left[1] + h , top_left[0]:top_left[0] + w ]\n",
    "        squares_cropped.append(square_cropped)\n",
    "#         res_values.append(res_max_value)\n",
    "\n",
    "#         cv2.rectangle(range_to_match_copy,top_left, bottom_right, 255, 2)   \n",
    "        \n",
    "    for k in range(3):\n",
    "        square_cropped = squares_cropped[k]\n",
    "        imgsquare_name = \"template\" + str(k) + \"_\" + range_name + \".png\"\n",
    "        cv2.imwrite(os.path.join(square_cropped_save_dir, imgsquare_name), square_cropped)\n",
    "\n",
    "#     imgname = range_name + \"_\" + str(int(res_max_value*100)) + \".png\"\n",
    "#     cv2.imwrite(os.path.join(range_save_dir, imgname), range_to_match_copy)\n",
    "    \n",
    "    return bboxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(image_paths, vocab_size):\n",
    "    \"\"\"\n",
    "      This function will sample SIFT descriptors from the training images,\n",
    "      cluster them with kmeans, and then return the cluster centers.\n",
    "\n",
    "      Args:\n",
    "      -   image_paths: list of image paths.\n",
    "      -   vocab_size: size of vocabulary\n",
    "\n",
    "      Returns:\n",
    "      -   vocab: This is a vocab_size x d numpy array (vocabulary). Each row is a\n",
    "          cluster center / visual word\n",
    "    \"\"\"\n",
    "    # Load images from the training set. To save computation time, you don't\n",
    "    # necessarily need to sample from all images, although it would be better\n",
    "    # to do so. You can randomly sample the descriptors from each image to save\n",
    "    # memory and speed up the clustering. Or you can simply call vl_dsift with\n",
    "    # a large step size here, but a smaller step size in get_bags_of_sifts.\n",
    "    #\n",
    "    # For each loaded image, get some SIFT features. You don't have to get as\n",
    "    # many SIFT features as you will in get_bags_of_sift, because you're only\n",
    "    # trying to get a representative sample here. You can try taking 20 features\n",
    "    # per image.\n",
    "    #\n",
    "    # Once you have tens of thousands of SIFT features from many training\n",
    "    # images, cluster them with kmeans. The resulting centroids are now your\n",
    "    # visual word vocabulary.\n",
    "\n",
    "    dim = 128      # length of the SIFT descriptors that you are going to compute.\n",
    "    vocab = np.zeros((vocab_size,dim))\n",
    "    total_SIFT_features = np.zeros((20*len(image_paths), dim))\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "\n",
    "    # raise NotImplementedError('`build_vocabulary` function needs to be implemented')\n",
    "    descriptor_collection = np.zeros((1, dim))\n",
    "    sample_img_paths = image_paths\n",
    "#     sample_img_paths = np.random.choice(image_paths, int(len(image_paths)/10))\n",
    "\n",
    "    for i in range(len(sample_img_paths)):\n",
    "    # for i in range(10):\n",
    "        img = cv2.imread(sample_img_paths[i], 0)\n",
    "        N = 50\n",
    "        # N = 4\n",
    "        step = max(int((img.shape[0] / N)), 1)\n",
    "        size = 4\n",
    "             \n",
    "        frames, descriptors = vlfeat.sift.dsift(img, step=step, size=size)\n",
    "        # print(descriptors.shape)\n",
    "        # descriptors = descriptors[:21, :]\n",
    "        descriptor_collection = np.vstack((descriptor_collection, descriptors))\n",
    "        \n",
    "    trimmed_descriptor_collection = descriptor_collection[1:,:]\n",
    "    cluster_centers = vlfeat.kmeans.kmeans(trimmed_descriptor_collection, vocab_size)\n",
    "    vocab = cluster_centers\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bags_of_sifts(image_paths, vocab_filename):\n",
    "    \"\"\"\n",
    "      You will want to construct SIFT features here in the same way you\n",
    "      did in build_vocabulary() (except for possibly changing the sampling\n",
    "      rate) and then assign each local feature to its nearest cluster center\n",
    "      and build a histogram indicating how many times each cluster was used.\n",
    "      Don't forget to normalize the histogram, or else a larger image with more\n",
    "      SIFT features will look very different from a smaller version of the same\n",
    "      image.\n",
    "\n",
    "      Args:\n",
    "      -   image_paths: paths to N images\n",
    "      -   vocab_filename: Path to the precomputed vocabulary.\n",
    "              This function assumes that vocab_filename exists and contains an\n",
    "              vocab_size x 128 ndarray 'vocab' where each row is a kmeans centroid\n",
    "              or visual word. This ndarray is saved to disk rather than passed in\n",
    "              as a parameter to avoid recomputing the vocabulary every run.\n",
    "\n",
    "      Returns:\n",
    "      -   image_feats: N x d matrix, where d is the dimensionality of the\n",
    "              feature representation. In this case, d will equal the number of\n",
    "              clusters or equivalently the number of entries in each image's\n",
    "              histogram (vocab_size) below.\n",
    "    \"\"\"\n",
    "    # load vocabulary\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # dummy features variable\n",
    "    feats = []\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "\n",
    "    # raise NotImplementedError('`get_bags_of_sifts` function needs to be implemented')\n",
    "    for i in range(len(image_paths)):\n",
    "    # for i in range(10):\n",
    "        img = cv2.imread(image_paths[i], 0)\n",
    "        N = 50\n",
    "        # N = 4\n",
    "        step = max(int((img.shape[0] / N)), 1)\n",
    "        size = 4\n",
    "             \n",
    "        frames, descriptors = vlfeat.sift.dsift(img, step=step, size=size)\n",
    "        D = cdist(descriptors, vocab)\n",
    "        indice_of_closest_vocab = np.argmin(D, axis=1)\n",
    "        histogram = np.zeros(vocab.shape[0])\n",
    "        for ind in indice_of_closest_vocab:\n",
    "            histogram[ind] += 1\n",
    "        normalized_histogram = normalize(histogram)\n",
    "        feats.append(normalized_histogram)\n",
    "        \n",
    "    feats = np.array(feats)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "    return feats\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    \"\"\"\n",
    "    This function will train a one-versus-one support vector machine (SVM)\n",
    "    and then use the learned classifiers to predict the category of every test image. \n",
    "\n",
    "    Args:\n",
    "    -   train_image_feats:  N x d numpy array, where d is the dimensionality of\n",
    "            the feature representation\n",
    "    -   train_labels: N element list, where each entry is a string indicating the\n",
    "            ground truth category for each training image\n",
    "    -   test_image_feats: M x d numpy array, where d is the dimensionality of the\n",
    "            feature representation. You can assume N = M, unless you have changed\n",
    "            the starter code\n",
    "    Returns:\n",
    "    -   test_labels: M element list, where each entry is a string indicating the\n",
    "            predicted category for each testing image\n",
    "    \"\"\"\n",
    "    categories = list(set(train_labels))\n",
    "    test_labels = []\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "    \n",
    "    # raise NotImplementedError('`svm_classify` function needs to be implemented')\n",
    "    svm = SVC(C=1000)\n",
    "    model = svm.fit(train_image_feats, train_labels)\n",
    "    test_labels = model.predict(test_image_feats)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "    return test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 65:\n",
      "--- 63.08299899101257 seconds ---\n",
      "image 66:\n",
      "--- 1.81504225730896 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(images)-5, len(images)-3):\n",
    "#     print(\"image \" + str(i) + \":\") \n",
    "\n",
    "#     img_0 = cv2.imread(images[i])\n",
    "\n",
    "#     result = get_suspected_waldo_stripe_region_for_img(img_0)\n",
    "#     raw_stripe_confidence_regions_df = get_raw_stripe_confidence_regions_df_from_suspected_stripe_regions(result)\n",
    "#     combined_stripe_confidence_regions_df = get_combined_stripe_confidence_regions_df_from_raw_stripe_confidence_regions_df(raw_stripe_confidence_regions_df)\n",
    "\n",
    "#     draw_rectangles_from_stripe_region_df(combined_stripe_confidence_regions_df, img_0, \"img_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-133-b651db5fc0f6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-133-b651db5fc0f6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \\textbb{Below are everything for one image}\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Below are everything for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 79.69399905204773 seconds ---\n",
      "--- 0.7400422096252441 seconds ---\n",
      "(48, 7)\n"
     ]
    }
   ],
   "source": [
    "num = 65\n",
    "img_0 = cv2.imread(images[num])\n",
    "\n",
    "range_save_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\range'\n",
    "range_square_save_dir = 'D:\\\\1920Sem1\\\\CS4243\\\\project_wx_tryout_code\\\\range_square'\n",
    "template_wanda = cv2.imread('template_wanda_2.png', 0)\n",
    "\n",
    "red_white_mask_rot = preprocess_input_img(img_0)\n",
    "\n",
    "print(\"image \" + str(num) + \":\") \n",
    "\n",
    "result = get_suspected_waldo_stripe_region_for_img(img_0)\n",
    "raw_stripe_confidence_regions_df = get_raw_stripe_confidence_regions_df_from_suspected_stripe_regions(result)\n",
    "combined_stripe_confidence_regions_df = get_combined_stripe_confidence_regions_df_from_raw_stripe_confidence_regions_df(raw_stripe_confidence_regions_df)\n",
    "\n",
    "df_for_rectangles = combined_stripe_confidence_regions_df.copy()\n",
    "\n",
    "df_for_head = df_for_rectangles[['row_confidence_interval_start', 'col_confidence_interval_start', 'row_confidence_interval_end', 'col_confidence_interval_end']].copy()\n",
    "df_for_head['stripe_width'] = (df_for_head['col_confidence_interval_end'] - df_for_head['col_confidence_interval_start'])/8\n",
    "\n",
    "# to expand the body bbox horizontally a bit and extend the body bbox upwards to bound the head\n",
    "df_for_head['col_confidence_interval_start'] = np.where(df_for_head['col_confidence_interval_start'] - 2*df_for_head['stripe_width']<0, 0, df_for_head['col_confidence_interval_start'] - 2*df_for_head['stripe_width'])\n",
    "df_for_head['col_confidence_interval_end'] = np.where(df_for_head['col_confidence_interval_end'] + 2*df_for_head['stripe_width']>img_0.shape[1]-1, img_0.shape[1]-1, df_for_head['col_confidence_interval_end'] + 2*df_for_head['stripe_width'])\n",
    "df_for_head['row_confidence_interval_start'] = np.where(df_for_head['row_confidence_interval_start'] - 10*df_for_head['stripe_width']<0, 0, df_for_head['row_confidence_interval_start'] - 10*df_for_head['stripe_width'])\n",
    "df_for_head['row_confidence_interval_end'] = np.where(df_for_head['row_confidence_interval_end'] + 1*df_for_head['stripe_width']>img_0.shape[0]-1, img_0.shape[0]-1, df_for_head['row_confidence_interval_end'] + 1*df_for_head['stripe_width'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_data = df_for_head[:].to_numpy()\n",
    "\n",
    "range_data_stripe_centre = df_for_rectangles[['col_no_original', 'stripe_centre_for_each_col']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(range_data.shape[0]):\n",
    "\n",
    "    wanda_row = tuple(range_data[i].astype(int))\n",
    "    wanda_row_start, wanda_col_start, wanda_row_end, wanda_col_end, wanda_stripe_width = wanda_row\n",
    "\n",
    "    range1 = img_0[wanda_row_start:wanda_row_end, wanda_col_start:wanda_col_end]\n",
    "    range1_gray = cv2.cvtColor(range1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    head_stripe_counts = [3, 5, 6]\n",
    "    for head_stripe_count in head_stripe_counts:\n",
    "        kernel_size = (head_stripe_count*wanda_stripe_width // 2) * 2 + 1\n",
    "\n",
    "        match_wanda_face_dir(template_wenda_dir, range1_gray, kernel_size, str(head_stripe_count) + \"_\" + str(num) + \"_range\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_cropped_save_dir = 'square_cropped_save_dir'\n",
    "identified_square_save_dir = 'identified_square_save_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in all_files_in_dir(square_cropped_save_dir):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = []\n",
    "for i in range(range_data.shape[0]):\n",
    "\n",
    "    wanda_row = tuple(range_data[i].astype(int))\n",
    "    wanda_row_start, wanda_col_start, wanda_row_end, wanda_col_end, wanda_stripe_width = wanda_row\n",
    "\n",
    "    range1 = img_0[wanda_row_start:wanda_row_end, wanda_col_start:wanda_col_end]\n",
    "    range1_gray = cv2.cvtColor(range1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    head_stripe_counts = [3, 5, 6]\n",
    "    \n",
    "    bboxes_of_row = []\n",
    "    \n",
    "    for j in range(len(head_stripe_counts)):\n",
    "        head_stripe_count = head_stripe_counts[j]\n",
    "        kernel_size = (head_stripe_count*wanda_stripe_width // 2) * 2 + 1\n",
    "        bboxes_of_three_templates = match_face(template_wenda_dir, img_0, wanda_row, kernel_size, \"countind\" + str(j) + \"_\" + \"img\" +  str(num) + \"_range\"+str(i), square_cropped_save_dir)\n",
    "        bboxes_of_row.append(bboxes_of_three_templates)\n",
    "        \n",
    "    \n",
    "    bboxes.append(bboxes_of_row)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_row_no_from_square_name(square_file_name):\n",
    "    square_file_name = square_file_name[:-4]\n",
    "    split_info = square_file_name.split(\"_\")\n",
    "    row_no = split_info[3][split_info[3].index('range') + len('range'):]\n",
    "    return int(row_no)\n",
    "\n",
    "def bbox_of_identified_square(square_file_name, bboxes):\n",
    "    square_file_name = square_file_name[:-4]\n",
    "    split_info = square_file_name.split(\"_\")\n",
    "    template_no = split_info[0][split_info[0].index('template') + len('template'):]\n",
    "    countind_no = split_info[1][split_info[1].index('countind') + len('countind'):]\n",
    "    row_no = split_info[3][split_info[3].index('range') + len('range'):]\n",
    "    \n",
    "    return bboxes[int(row_no)][int(countind_no)][int(template_no)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wenda_square_save_dir = 'range_square\\\\wenda'\n",
    "wenda_waldo_square_save_dir = 'range_square\\\\wenda_waldo'\n",
    "rubbish_square_save_dir = 'range_square\\\\rubbish'\n",
    "\n",
    "wenda_squares = all_files_in_dir(wenda_square_save_dir)\n",
    "wenda_waldo_squares = all_files_in_dir(wenda_waldo_square_save_dir)\n",
    "rubbish_squares = all_files_in_dir(rubbish_square_save_dir)\n",
    "\n",
    "test_squares = all_files_in_dir(square_cropped_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_filename = 'vocab.pkl'\n",
    "if not os.path.isfile(vocab_filename):\n",
    "    print('No existing visual word vocabulary found. Computing one from training images')\n",
    "    vocab_size = 100  # Larger values will work better (to a point) but be slower to compute\n",
    "    vocab = build_vocabulary(wenda_squares, vocab_size)\n",
    "    \n",
    "    with open(vocab_filename, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "        print('{:s} saved'.format(vocab_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = [i for i in range(400)]\n",
    "if (len(rubbish_squares) > 400):\n",
    "    random_index = random.sample(range(len(rubbish_squares)), 400)\n",
    "    \n",
    "train_imgs = wenda_squares + [rubbish_squares[i] for i in random_index] \n",
    "train_labels = ['wenda' for i in range(len(wenda_squares))] + [\"rubbish\" for i in random_index] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feats = bags_of_sifts(train_imgs, 'vocab.pkl')\n",
    "test_feats = bags_of_sifts(test_squares, 'vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square_cropped_save_dir\\template0_countind0_img65_range36.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1920sem1\\cs4243\\cs4243-lab3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test_labels = svm_classify(train_feats, train_labels, test_feats)\n",
    "detected_wendas = [test_squares[i] for i in range(len(test_labels)) if test_labels[i]=='wenda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in all_files_in_dir(identified_square_save_dir):\n",
    "    os.remove(f)\n",
    "    \n",
    "for detected_wenda in detected_wendas:\n",
    "    copyfile(detected_wenda, os.path.join(identified_square_save_dir, path_leaf(detected_wenda)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in all_files_in_dir(square_cropped_save_dir):\n",
    "    os.remove(f)\n",
    "\n",
    "for f in all_files_in_dir(identified_square_save_dir):\n",
    "    copyfile(f, os.path.join(square_cropped_save_dir, path_leaf(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_imgs = wenda_squares + wenda_waldo_squares \n",
    "train_labels = ['wenda' for i in range(len(wenda_squares))] + [\"waldo\" for i in range(len(wenda_waldo_squares))] \n",
    "test_squares = all_files_in_dir(square_cropped_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feats = bags_of_sifts(train_imgs, 'vocab.pkl')\n",
    "test_feats = bags_of_sifts(test_squares, 'vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1920sem1\\cs4243\\cs4243-lab3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test_labels = svm_classify(train_feats, train_labels, test_feats)\n",
    "detected_wendas = [test_squares[i] for i in range(len(test_labels)) if test_labels[i]=='wenda']\n",
    "for f in all_files_in_dir(identified_square_save_dir):\n",
    "    os.remove(f)\n",
    "    \n",
    "for detected_wenda in detected_wendas:\n",
    "    copyfile(detected_wenda, os.path.join(identified_square_save_dir, path_leaf(detected_wenda)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'template1_countind0_img65_range36.png'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_leaf(detected_wenda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1438, 4430, 51, 51)\n",
      "[4282.14285714 1391.28571429 4648.14285714 1600.42857143   17.42857143]\n",
      "[1495.85714286 4543.57142857]\n",
      "(6590, 5980, 25, 25)\n",
      "[5.9035e+03 6.5830e+03 6.0085e+03 6.6430e+03 5.0000e+00]\n",
      "[6613.  5978.5]\n",
      "(1444, 4432, 51, 51)\n",
      "[4282.14285714 1391.28571429 4648.14285714 1600.42857143   17.42857143]\n",
      "[1495.85714286 4543.57142857]\n"
     ]
    }
   ],
   "source": [
    "for f in all_files_in_dir(identified_square_save_dir):\n",
    "    bbox_of_square = bbox_of_identified_square(path_leaf(f), bboxes)\n",
    "    row_no_of_square = get_row_no_from_square_name(path_leaf(f))\n",
    "    # bbox_of_range: [row_start, col_start, row_end, col_end, stripe_width]\n",
    "    bbox_of_range = range_data[row_no_of_square]\n",
    "    # centre_of_stripes_in_range: ['col_no_original', 'stripe_centre_for_each_col']\n",
    "    centre_of_stripes_in_range = range_data_stripe_centre[:].to_numpy()[row_no_of_square]\n",
    "    print(bbox_of_square)\n",
    "    print(bbox_of_range)\n",
    "    print(centre_of_stripes_in_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
